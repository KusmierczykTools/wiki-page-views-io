
The project is devoted to support Wikipedia page view dumps IO.

-----------------------------------------------------------------------------------------------------------------

Source data:

    The GZ files can be downloaded from:
    http://dumps.wikimedia.org/other/pagecounts-raw/    
    You can use: `sh deploy/wiki_pageviews_download.sh YEAR MONTH` for that.
    WARNING: These files need to be manualy fixed that there is exactly one file per hour.

    List of allowed wiki pages can be downloaded from:
    http://dumps.wikimedia.org/enwiki/20140102/enwiki-20140102-pages-articles-multistream-index.txt.bz2

-----------------------------------------------------------------------------------------------------------------

To prepare list of allowed wikipedia pages (should be done in deploy directory):

     wget http://dumps.wikimedia.org/enwiki/20140102/enwiki-20140102-pages-articles-multistream-index.txt.bz2
     bzip2 -d enwiki-20140102-pages-articles-multistream-index.txt.bz2
     cut -d ":" enwiki-20140102-pages-articles-multistream-index.txt -f 3 | python create_list_of_variants.py > /tmp/wikipedia_variants.txt

The script above unpacks and expands list of page names in a way that each possible variant is included.

-----------------------------------------------------------------------------------------------------------------

To transform WikiPageViews gz archives to more readable form we first need a list of files:

    cd deploy
    ls DIRECTORY_WITH_GZ_FILES/*.gz > /tmp/list

To validate (size and dates) list of files:

    sh wiki_pageviews_validation.sh /tmp/list

To preprocess files:

    sh wiki_pageviews_preprocess.sh /tmp/list /tmp/temporal_storage /tmp/wikipedia_variants.txt

To transform and pack files into storage:

    ls /tmp/temporal_storage/*out > /tmp/list
    sh wiki_pageviews_pack.sh  /tmp/list OUTPUT-STORAGE-PATH

All above steps can be run on as many SUBSEQUENT list as you wish e.g. you can create separate list for each year
and run one after the other.
At the end you should run:

    sh wiki_pageviews_pack_finalize.sh OUTPUT-STORAGE-PATH
    #OR: sh wiki_pageviews_pack_finalize.sh OUTPUT-STORAGE-PATH full 
    #    (if you don't want anything in meta.txt file)

-----------------------------------------------------------------------------------------------------------------

If you want to process only some part of entries (e.g. only entries with names starting with 'a')
you can remove some shards from list_of_shards.txt file. 

WARNING: there is one process created per shard so be careful with things that are in 
file_sharding.sh (number of shards) and list_of_shards.txt (list of shards that should be processed on the node). 

To edit cache length: files_processing.sh
To edit number of shards: file_sharding.sh and list_of_shards.txt



